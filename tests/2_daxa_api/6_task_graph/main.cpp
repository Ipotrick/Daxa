#pragma once

#include "common.hpp"
DAXA_DECL_COMPUTE_TASK_HEAD_BEGIN(ExampleTaskHead)
DAXA_TH_BUFFER(READ, buffer0)
DAXA_TH_IMAGE(SAMPLED, REGULAR_2D, image0)
DAXA_TH_IMAGE(SAMPLED, REGULAR_2D, image1)
DAXA_TH_BUFFER(READ, test_buffer_no_shader)
DAXA_DECL_TASK_HEAD_END

void example_task_callback(daxa::TaskInterface ti)
{
    auto const & AI = ExampleTaskHead::Info::AT;

    // There are two ways to get the info for any attachment:
    {
        // daxa::TaskBufferAttachmentIndex index:
        [[maybe_unused]] daxa::TaskBufferAttachmentInfo const & buffer0_attachment0 = ti.get(AI.buffer0);
        // daxa::TaskBufferView assigned to the buffer attachment:
        [[maybe_unused]] daxa::TaskBufferAttachmentInfo const & buffer0_attachment1 = ti.get(buffer0_attachment0.view);
    }
    // The Buffer Attachment info contents:
    {
        [[maybe_unused]] daxa::BufferId id = ti.get(AI.buffer0).ids[0];
        [[maybe_unused]] char const * name = ti.get(AI.buffer0).name;
        [[maybe_unused]] daxa::TaskAccess access = ti.get(AI.buffer0).task_access;
        [[maybe_unused]] u8 shader_array_size = ti.get(AI.buffer0).shader_array_size;
        [[maybe_unused]] bool shader_as_address = ti.get(AI.buffer0).shader_as_address;
        [[maybe_unused]] daxa::TaskBufferView view = ti.get(AI.buffer0).view;
        [[maybe_unused]] std::span<daxa::BufferId const> ids = ti.get(AI.buffer0).ids;
    }
    // The Image Attachment info contents:
    {
        [[maybe_unused]] char const * name = ti.get(AI.image0).name;
        [[maybe_unused]] daxa::TaskAccess access = ti.get(AI.image0).task_access;
        [[maybe_unused]] daxa::ImageViewType view_type = ti.get(AI.image0).view_type;
        [[maybe_unused]] u8 shader_array_size = ti.get(AI.image0).shader_array_size;
        [[maybe_unused]] daxa::TaskHeadImageArrayType shader_array_type = ti.get(AI.image0).shader_array_type;
        [[maybe_unused]] daxa::ImageLayout layout = ti.get(AI.image0).layout;
        [[maybe_unused]] daxa::TaskImageView view = ti.get(AI.image0).view;
        [[maybe_unused]] std::span<daxa::ImageId const> ids = ti.get(AI.image0).ids;
        [[maybe_unused]] std::span<daxa::ImageViewId const> view_ids = ti.get(AI.image0).view_ids;
    }
    // The interface has multiple convenience functions for easier access to the underlying resources attributes:
    {
        // Overloaded for buffer, blas, tlas, image
        [[maybe_unused]] daxa::BufferInfo info = ti.info(AI.buffer0).value();
        // Overloaded for buffer, blas, tlas
        [[maybe_unused]] daxa::DeviceAddress address = ti.device_address(AI.buffer0).value();

        [[maybe_unused]] std::byte * host_address = ti.buffer_host_address(AI.buffer0).value();
        [[maybe_unused]] daxa::ImageViewInfo img_view_info = ti.image_view_info(AI.image0).value();

        // In case the task resource has an array of real resources, one can use the optional second parameter to access those:
        [[maybe_unused]] daxa::BufferInfo info2 = ti.info(AI.buffer0, 123 /*resource index*/).value();
    }
    // The attachment infos are also provided, directly via a span:
    for ([[maybe_unused]] daxa::TaskAttachmentInfo const & attach : ti.attachment_infos)
    {
    }
    // The tasks shader side struct of ids and addresses is automatically filled and serialized to a blob:
    [[maybe_unused]] auto generated_blob = ti.attachment_shader_blob;
    // The head also declared an aligned struct with the right size as a dummy on the c++ side.
    // This can be used to declare shader/c++ shared structs containing this blob :
    [[maybe_unused]] ExampleTaskHead::AttachmentShaderBlob blob = {};
    // The blob also declares a constructor and assignment operator to take in the byte span generated by the taskgraph:
    blob = generated_blob;
    [[maybe_unused]] ExampleTaskHead::AttachmentShaderBlob blob2{ti.attachment_shader_blob};
}

static auto EXAMPLE_TASK = daxa::Task().uses_head<ExampleTaskHead::Info>().executes(example_task_callback);

void test_task_copy_and_move()
{
    daxa::Task t = EXAMPLE_TASK;
    [[maybe_unused]] daxa::Task t2 = t;
    std::optional<daxa::Task> t3;
    t3.emplace(t);
    t3.reset();
    [[maybe_unused]] daxa::Task t4 = std::move(t);
}

#include "mipmapping.hpp"
#include "shaders/shader_integration.inl"
#include "persistent_resources.hpp"
#include "transient_overlap.hpp"

namespace daxa
{
    using Task = InlineTask;
}

namespace tests
{
    void head_task_syntax_external_callback(daxa::TaskInterface ti, float f, int i)
    {
        auto const & AT = ExampleTaskHead::AT;
        [[maybe_unused]] auto id = ti.id(AT.buffer0);

        printf("sum: %f\n", f + i);
    }

    struct OldTaskHeadSyntaxTask : ExampleTaskHead::Task
    {
        float f = {};
        int i = {};
        AttachmentViews views = {};
        void callback(daxa::TaskInterface ti)
        {
            auto const & AT = ExampleTaskHead::AT;
            [[maybe_unused]] auto id = ti.id(AT.buffer0);

            printf("sum: %f\n", f + i);
        }
    };

    void head_task_syntax()
    {
        float f = 3.14f;
        int i = 5;

        auto old_task_syntax = OldTaskHeadSyntaxTask{
            .views = OldTaskHeadSyntaxTask::Views{
                .buffer0 = {},
                .image0 = {},
                .image1 = {},
                .test_buffer_no_shader = {},
            },
            .f = f,
            .i = i,
        };

        daxa::TaskBufferView cmd_view = daxa::NullTaskBuffer;
        daxa::TaskBufferView buf_view2 = daxa::NullTaskBuffer;
        daxa::TaskBufferView buf_view = daxa::NullTaskBuffer;
        daxa::TaskImageView img_view = daxa::NullTaskImage;
        daxa::TaskImageView img_view2 = daxa::NullTaskImage;

        auto new_task_syntax = daxa::Task::Compute("Name")
            .uses_head<ExampleTaskHead::Info>()
            .head_views({ .image1 = img_view2 })                            // No overwrite of access or stage, uses the task head declared access
            .transfer.reads({ .buffer0 = buf_view })                        // Overrides the access type of head attachment, also overrides the stage to that of the task (in this case compute)
            .writes(daxa::TaskStage::RASTER_SHADER, {.image0 = img_view })  // Overrides the access type of head attachment, also overrides the stage to RASTER_SHADER
            .indirect_cmd.reads(cmd_view)                                   // regular old inline attachment that is not part of the head
            .reads(daxa::TaskStage::COMPUTE_SHADER, buf_view2)              // regular old inline attachment that is not part of the head
            .executes(head_task_syntax_external_callback, f, i);

    }

    void simplest()
    {
        AppContext const app = {};
        auto d = app.device;
        struct S
        {
            daxa::Device d;
        } s = {d};
        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .name = APPNAME_PREFIX("task_graph (simplest)"),
        });
    }

    void execution()
    {
        AppContext const app = {};
        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .name = APPNAME_PREFIX("task_graph (execution)"),
        });

        // This is pointless, but done to show how the task graph executes
        task_graph.add_task(
            daxa::InlineTask(APPNAME_PREFIX("task 1 (execution)"))
                .executes(
                    [=](daxa::TaskInterface)
                    {
                        std::cout << "Hello, ";
                    }));

        task_graph.add_task(
            daxa::InlineTask(APPNAME_PREFIX("task 2 (execution)"))
                .executes(
                    [=](daxa::TaskInterface)
                    {
                        std::cout << "World!" << std::endl;
                    }));

        task_graph.complete({});

        task_graph.execute({});
    }

    void write_read_image()
    {
        // TEST:
        //    1) CREATE image
        //    2) WRITE image
        //    3) READ image
        AppContext app = {};
        // Need to scope the task graphs lifetime.
        // Task graph MUST die before we call wait_idle and collect_garbage.
        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .record_debug_information = true,
            .name = APPNAME_PREFIX("create-write-read image"),
        });
        // CREATE IMAGE
        auto task_image = task_graph.create_transient_image(daxa::TaskTransientImageInfo{.size = {1, 1, 1}, .name = "task graph tested image"});
        // WRITE IMAGE 1
        task_graph.add_task(daxa::InlineTask::Compute("write image 1")
                                .writes(task_image)
                                .executes([](daxa::TaskInterface) {}));
        // READ_IMAGE 1
        task_graph.add_task(daxa::InlineTask::Compute("read image 1")
                                .reads(task_image)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.complete({});
        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;
    }

    void write_read_image_layer()
    {
        // TEST:
        //    1) CREATE image
        //    2) WRITE into array layer 1 of the image
        //    3) READ from array layer 2 of the image
        AppContext app = {};
        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .record_debug_information = true,
            .name = APPNAME_PREFIX("create-write-read array layer"),
        });
        // CREATE IMAGE
        auto task_image = task_graph.create_transient_image({
            .size = {1, 1, 1},
            .array_layer_count = 2,
            .name = "task graph tested image",
        });
        auto timg_view_l0 = task_image.layers(0);
        task_graph.add_task(
            daxa::InlineTask::Compute("write image array layer 1")
                .writes(timg_view_l0)
                .executes([](daxa::TaskInterface) {}));
        // READ_IMAGE 1
        auto timg_view_l1 = task_image.layers(1);
        task_graph.add_task(
            daxa::InlineTask::Compute("read image array layer 1")
                .samples(timg_view_l1)
                .executes([](daxa::TaskInterface) {}));
        task_graph.complete({});
        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;
    }

    void create_transfer_read_buffer()
    {
        // TEST:
        //    1) CREATE buffer
        //    2) TRANSFER into the buffer
        //    3) READ from the buffer
        AppContext app = {};
        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .record_debug_information = true,
            .name = APPNAME_PREFIX("create-transfer-read buffer"),
        });

        auto task_buffer = task_graph.create_transient_buffer({
            .size = sizeof(u32),
            .name = "task graph tested buffer",
        });

        task_graph.add_task(daxa::InlineTask::Transfer("host transfer buffer")
                                .host.writes(task_buffer)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Compute("read buffer")
                .reads(task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.complete({});
        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;
    }

    void initial_layout_access()
    {
        // TEST:
        //    1) CREATE image - set the task image initial access to write from compute shader
        //    2) READ from a the subimage
        //    3) WRITE into the subimage
        AppContext app = {};
        auto image = app.device.create_image({
            .size = {1, 1, 1},
            .array_layer_count = 2,
            .usage = daxa::ImageUsageFlagBits::SHADER_STORAGE | daxa::ImageUsageFlagBits::SHADER_SAMPLED,
            .name = APPNAME_PREFIX("tested image"),
        });

        std::array init_access = {
            daxa::ImageSliceState{
                .latest_access = daxa::AccessConsts::COMPUTE_SHADER_WRITE,
                .latest_layout = daxa::ImageLayout::GENERAL,
            },
        };

        auto task_image = daxa::TaskImage(daxa::TaskImageInfo{
            .initial_images = {
                .images = {&image, 1},
                .latest_slice_states = {init_access.data(), 1}},
            .swapchain_image = false,
            .name = "task graph tested image",
        });

        // TG MUST die before image, as it holds image views to the image that must die before the image.
        {
            auto task_graph = daxa::TaskGraph({
                .device = app.device,
                .record_debug_information = true,
                .name = APPNAME_PREFIX("initial layout image"),
            });
            // CREATE IMAGE
            task_graph.use_persistent_image(task_image);
            task_graph.add_task(daxa::InlineTask::Compute("read image layer 1")
                                    .samples(task_image.view().layers(1))
                                    .executes([](daxa::TaskInterface) {}));
            task_graph.add_task(daxa::InlineTask::Compute("write image layer 1")
                                    .writes(task_image.view().layers(0))
                                    .executes([](daxa::TaskInterface) {}));
            task_graph.complete({});
            task_graph.execute({});
            std::cout << task_graph.get_debug_string() << std::endl;
        }
        app.device.destroy_image(image);
    }

    void tracked_slice_barrier_collapsing()
    {
        // TEST:
        //    1) CREATE image - set the task image initial access to write from compute
        //                      shader for one subresouce and read for the other
        //    2) WRITE into the subsubimage
        //    3) READ from a the subsubimage
        //    4) WRITE into the entire image
        //    5) READ the entire image
        //    Expected: There should only be a single barrier between tests 4 and 5.
        AppContext app = {};
        auto image = app.device.create_image({
            .size = {1, 1, 1},
            .array_layer_count = 4,
            .usage = daxa::ImageUsageFlagBits::SHADER_STORAGE | daxa::ImageUsageFlagBits::SHADER_SAMPLED,
            .name = APPNAME_PREFIX("tested image"),
        });

        // CREATE IMAGE
        std::array init_access = {
            daxa::ImageSliceState{
                .latest_access = daxa::AccessConsts::COMPUTE_SHADER_WRITE,
                .latest_layout = daxa::ImageLayout::GENERAL,
                .slice = {.base_array_layer = 0, .layer_count = 2},
            },
            daxa::ImageSliceState{
                .latest_access = daxa::AccessConsts::COMPUTE_SHADER_READ,
                .latest_layout = daxa::ImageLayout::GENERAL,
                .slice = {.base_array_layer = 2, .layer_count = 2},
            }};
        auto task_image = daxa::TaskImage({
            .name = "task graph tested image",
        });

        // TG MUST die before image, as it holds image views to the image that must die before the image.
        {
            auto task_graph = daxa::TaskGraph({
                .device = app.device,
                .record_debug_information = true,
                .name = APPNAME_PREFIX("tracked slice barrier collapsing"),
            });

            task_image.set_images({.images = {&image, 1}, .latest_slice_states = {init_access.begin(), init_access.end()}});

            task_graph.use_persistent_image(task_image);

            task_graph.add_task(daxa::InlineTask::Compute("samples image layer 1")
                                    .samples(task_image.view().layers(1))
                                    .executes([](daxa::TaskInterface) {}));

            task_graph.add_task(daxa::InlineTask::Compute("write image layer 3")
                                    .writes(task_image.view().layers(3))
                                    .executes([](daxa::TaskInterface) {}));

            task_graph.add_task(daxa::InlineTask::Compute("write image layer 0 - 1")
                                    .writes(task_image.view().layers(0, 2))
                                    .executes([](daxa::TaskInterface) {}));

            task_graph.add_task(daxa::InlineTask::Compute("read image layer 0 - 3")
                                    .samples(task_image.view().layers(0, 4))
                                    .executes([](daxa::TaskInterface) {}));

            task_graph.complete({});
            task_graph.execute({});
            std::cout << task_graph.get_debug_string() << std::endl;
        }
        app.device.destroy_image(image);
    }

    void shader_integration_inl_use()
    {
        // TEST:
        //  1) Create resources
        //  2) Use Compute dispatch to write to image
        //  4) readback and validate
        AppContext app = {};
        auto dummy = app.device.create_image({
            .size = {16, 16, 1},
            .array_layer_count = 1,
            .usage = daxa::ImageUsageFlagBits::SHADER_STORAGE | daxa::ImageUsageFlagBits::TRANSFER_SRC,
            .name = "dummy",
        });
        auto image = app.device.create_image({
            .size = {16, 16, 1},
            .array_layer_count = 1,
            .usage = daxa::ImageUsageFlagBits::SHADER_STORAGE | daxa::ImageUsageFlagBits::TRANSFER_SRC,
            .name = "underlying image",
        });
        auto task_image = daxa::TaskImage({
            // In this test, this image name will be "aliased", so the name must not be the same.
            .initial_images = {
                .images = {&image, 1},
            },
            .name = "image",
        });
        auto buffer = app.device.create_buffer({
            .size = 16,
            .allocate_info = daxa::MemoryFlagBits::HOST_ACCESS_SEQUENTIAL_WRITE,
            .name = "underlying buffer",
        });
        *app.device.buffer_host_address_as<float>(buffer).value() = 0.75f;
        auto task_buffer = daxa::TaskBuffer({
            .initial_buffers = {
                .buffers = {&buffer, 1},
                .latest_access = daxa::AccessConsts::HOST_WRITE,
            },
            .name = "settings", // This name MUST be identical to the name used in the shader.
        });

        daxa::PipelineManager pipeline_manager = daxa::PipelineManager({
            .device = app.device,
            .root_paths = {
                DAXA_SHADER_INCLUDE_DIR,
                "tests/2_daxa_api/6_task_graph/shaders",
            },
            .name = "pipeline manager",
        });

        auto compile_result = pipeline_manager.add_compute_pipeline2({
            .source = daxa::ShaderFile{"shader_integration.glsl"},
            .enable_debug_info = true,
            .name = "compute_pipeline",
        });
        auto compute_pipeline = compile_result.value();

        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .record_debug_information = true,
            .name = "shader integration test - task graph",
        });
        task_graph.use_persistent_image(task_image);
        task_graph.use_persistent_buffer(task_buffer);

        struct WriteImage : ShaderIntegrationTaskHead::Task
        {
            AttachmentViews views = {};
            std::shared_ptr<daxa::ComputePipeline> pipeline = {};
            void callback(daxa::TaskInterface ti)
            {
                ti.recorder.set_pipeline(*pipeline);
                ti.recorder.push_constant_vptr({ti.attachment_shader_blob.data(), ti.attachment_shader_blob.size()});
                ti.recorder.dispatch({1, 1, 1});
            }
        };
        using namespace ShaderIntegrationTaskHead;
        task_graph.add_task(WriteImage{
            .views = WriteImage::Views{
                .settings = task_buffer.view(),
                .image = task_image.view(),
            },
            .pipeline = compute_pipeline,
        });
        task_graph.add_task(WriteImage{
            .views = WriteImage::Views{
                .settings = task_buffer.view(),
                .image = task_image.view(),
            },
            .pipeline = compute_pipeline,
        });
        task_graph.submit({});

        task_graph.complete({});
        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;
        app.device.wait_idle();
        app.device.destroy_image(image);
        app.device.destroy_image(dummy);
        app.device.destroy_buffer(buffer);
        app.device.collect_garbage();
    }

    void correct_read_buffer_task_ordering()
    {
        // TEST:
        //  1) Create persistent image and persistent buffer
        //  2) Record two task graphs A
        //  3) Task graph A has three tasks inserted in listed order:
        //      Task 1) Writes image
        //      Task 2) Reads image and reads buffer
        //      Task 3) Reads buffer
        //  5) Execute task graph and check the ordering of tasks in batches
        //  Expected result:
        //      NOTE(msakmary): This does something different currently (task 3 is in batch 2)
        //                      - this is due to limitations of what task graph can do without having a proper render-graph
        //                      - will be fixed in the future by adding JIRA
        //      Batch 1:
        //          Task 1
        //          Task 3
        //      Batch 2:
        //          Task 2
        daxa::Instance daxa_ctx = daxa::create_instance({});
        daxa::Device device = daxa_ctx.create_device_2(daxa_ctx.choose_device({}, {}));
        auto image = device.create_image({
            .size = {1, 1, 1},
            .array_layer_count = 1,
            .usage = daxa::ImageUsageFlagBits::SHADER_STORAGE | daxa::ImageUsageFlagBits::SHADER_SAMPLED,
            .name = "actual image",
        });

        auto buffer = device.create_buffer({
            .size = 1,
            .name = "actual_buffer",
        });

        auto persistent_task_image = daxa::TaskImage(daxa::TaskImageInfo{
            .initial_images = {.images = {&image, 1}},
            .swapchain_image = false,
            .name = "image",
        });

        auto persistent_task_buffer = daxa::TaskBuffer(daxa::TaskBufferInfo{
            .initial_buffers = {.buffers = {&buffer, 1}},
            .name = "buffer",
        });

        auto task_graph = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task_graph",
        });

        task_graph.use_persistent_image(persistent_task_image);
        task_graph.use_persistent_buffer(persistent_task_buffer);
        task_graph.add_task(daxa::InlineTask::Compute("write persistent image")
                                .writes(persistent_task_image)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Compute("read persistent image, read persistent buffer")
                                .reads(persistent_task_image)
                                .reads(persistent_task_buffer)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Raster("read persistent buffer")
                                .raster_shader.reads(persistent_task_buffer)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.submit({});
        task_graph.complete({});

        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;

        device.wait_idle();
        device.destroy_image(image);
        device.destroy_buffer(buffer);
        device.collect_garbage();
    }

    void test_concurrent_read_write_buffer()
    {
        // TEST:
        //  1) Create persistent buffer
        //  2) Record single task graph A
        //  3) Task graph A has four tasks inserted in listed order:
        //      Task 1) Writes buffer
        //      Task 2) Concurrent read write buffer
        //      Task 3) Concurrent read write buffer
        //      Task 4) Reads buffer
        //  5) Execute task graph and check the generated barriers and batches
        //  Expected result:
        //  Batch 0:
        //      Task 1)
        //  Batch 1:
        //      [Barrier Write -> Read Write] + Task 2) and 3)
        //  Batch 2:
        //      [Barrier Write -> Read] + Task 4)
        daxa::Instance daxa_ctx = daxa::create_instance({});
        daxa::Device device = daxa_ctx.create_device_2(daxa_ctx.choose_device({}, {}));

        auto buffer = device.create_buffer({
            .size = 1,
            .name = "actual_buffer",
        });

        auto persistent_task_buffer = daxa::TaskBuffer(daxa::TaskBufferInfo{
            .initial_buffers = {.buffers = {&buffer, 1}},
            .name = "buffer",
        });

        auto task_graph = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task_graph",
        });

        task_graph.use_persistent_buffer(persistent_task_buffer);

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 1) concurrent read write buffer")
                .raster_shader.writes(persistent_task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 2) concurrent write read buffer")
                .raster_shader.reads_writes_concurrent(persistent_task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 3) concurrent write read buffer")
                .raster_shader.reads_writes_concurrent(persistent_task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 4) read buffer")
                .raster_shader.reads(persistent_task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.submit({});
        task_graph.complete({});

        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;

        device.wait_idle();
        device.destroy_buffer(buffer);
        device.collect_garbage();
    }

    void test_concurrent_read_write_image()
    {
        // TEST:
        //  1) Create persistent buffer
        //  2) Record single task graph A
        //  3) Task graph A has four tasks inserted in listed order:
        //      Task 1) Read image
        //      Task 2) Concurrent read write image
        //      Task 3) Concurrent read write image
        //      Task 4) writes image
        //  5) Execute task graph and check the generated barriers and batches
        //  Expected result:
        //  Batch 0:
        //      Task 1)
        //  Batch 1:
        //      [Barrier Read -> Read Write] + Task 2) and 3)
        //  Batch 2:
        //      [Barrier Read Write -> Write] + Task 4)
        daxa::Instance daxa_ctx = daxa::create_instance({});
        daxa::Device device = daxa_ctx.create_device_2(daxa_ctx.choose_device({}, {}));
        auto image = device.create_image({
            .size = {1, 1, 1},
            .array_layer_count = 1,
            .usage = daxa::ImageUsageFlagBits::SHADER_STORAGE | daxa::ImageUsageFlagBits::SHADER_SAMPLED,
            .name = "actual image",
        });

        auto persistent_task_image = daxa::TaskImage(daxa::TaskImageInfo{
            .initial_images = {.images = {&image, 1}},
            .swapchain_image = false,
            .name = "image",
        });

        auto task_graph = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task_graph",
        });

        task_graph.use_persistent_image(persistent_task_image);

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 1) read image")
                .vertex_shader.reads(persistent_task_image)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 2) concurrent write read image")
                .vertex_shader.reads_writes_concurrent(persistent_task_image)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 3) concurrent write read image")
                .vertex_shader.reads_writes_concurrent(persistent_task_image)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("Task 4) write image")
                .vertex_shader.writes(persistent_task_image)
                .executes([](daxa::TaskInterface) {}));

        task_graph.submit({});
        task_graph.complete({});

        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;

        device.wait_idle();
        device.destroy_image(image);
        device.collect_garbage();
    }

    void test_concurrent_read_write_buffer_cross_graphs()
    {
        // TEST:
        //  1) Create persistent buffer
        //  2) Record two task graphs A and B
        //  3) Task graph A has one task inserted in listed order:
        //      Task 1) Concurrent read write buffer
        //  4) Task graph B has one task inserted in listed order:
        //      Task 1) Concurrent read write buffer
        //  5) Execute task graphs in the order A -> B and check the jit generated barriers:
        //  Expected result:
        //      Oversync between task graph A and B aka one jit [ReadWrite -> ReadWrite] Barrier between
        daxa::Instance daxa_ctx = daxa::create_instance({});
        daxa::Device device = daxa_ctx.create_device_2(daxa_ctx.choose_device({}, {}));

        auto buffer = device.create_buffer({
            .size = 1,
            .name = "actual_buffer",
        });

        auto persistent_task_buffer = daxa::TaskBuffer(daxa::TaskBufferInfo{
            .initial_buffers = {.buffers = {&buffer, 1}},
            .name = "buffer",
        });

        auto task_graph_A = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task graph A",
        });

        auto task_graph_B = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task graph B",
        });

        task_graph_A.use_persistent_buffer(persistent_task_buffer);
        task_graph_A.add_task(
            daxa::InlineTask::Raster("Task 1) concurrent write read buffer")
                .raster_shader.reads_writes_concurrent(persistent_task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph_A.submit({});
        task_graph_A.complete({});

        task_graph_B.use_persistent_buffer(persistent_task_buffer);
        task_graph_B.add_task(
            daxa::InlineTask::Raster("Task 1) concurrent write read buffer")
                .raster_shader.reads_writes_concurrent(persistent_task_buffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph_B.submit({});
        task_graph_B.complete({});

        task_graph_A.execute({});
        std::cout << task_graph_A.get_debug_string() << std::endl;
        task_graph_B.execute({});
        std::cout << task_graph_B.get_debug_string() << std::endl;

        device.wait_idle();
        device.destroy_buffer(buffer);
        device.collect_garbage();
    }

    void read_on_readwriteconcurrent()
    {
        // TEST:
        //  1) read write concurrent buffer
        //  2) read write concurrent buffer
        //  3) read buffer
        // EXPECTED:
        // batch 0 : read write concurrent (1) (2)
        // batch 2 : read buffer (3)
        daxa::Instance daxa_ctx = daxa::create_instance({});
        daxa::Device device = daxa_ctx.create_device_2(daxa_ctx.choose_device({}, {}));

        auto buffer = device.create_buffer({
            .size = 1,
            .name = "actual_buffer",
        });

        auto tbuffer = daxa::TaskBuffer(daxa::TaskBufferInfo{
            .initial_buffers = {.buffers = {&buffer, 1}},
            .name = "buffer",
        });

        auto task_graph = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task graph",
        });

        task_graph.use_persistent_buffer(tbuffer);

        task_graph.add_task(
            daxa::InlineTask::Compute("write 0")
                .writes(tbuffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Compute("read write concurrent 1")
                .reads_writes_concurrent(tbuffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Compute("read write concurrent 2")
                .reads_writes_concurrent(tbuffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Raster("read")
                .raster_shader.reads(tbuffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.submit({});
        task_graph.complete({});

        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;

        device.wait_idle();
        device.destroy_buffer(buffer);
        device.collect_garbage();
    }

    void concurrent_read_on_read()
    {
        // TEST:
        //  1) write buffer
        //  2) read buffer
        //  3) read buffer
        //  4) write buffer
        // EXPECTED:
        // batch 0 : write buffer (1)
        // batch 1 : read buffer (2 and 3)
        // batch 2 : wriite buffer (4)
        daxa::Instance daxa_ctx = daxa::create_instance({});
        daxa::Device device = daxa_ctx.create_device_2(daxa_ctx.choose_device({}, {}));

        auto buffer = device.create_buffer({
            .size = 1,
            .name = "actual_buffer",
        });

        auto persistent_task_buffer = daxa::TaskBuffer(daxa::TaskBufferInfo{
            .initial_buffers = {.buffers = {&buffer, 1}},
            .name = "buffer",
        });

        auto buffer_b = daxa::TaskBuffer(daxa::TaskBufferInfo{
            .initial_buffers = {.buffers = {&buffer, 1}},
            .name = "buffer b",
        });

        auto task_graph = daxa::TaskGraph({
            .device = device,
            .record_debug_information = true,
            .name = "task graph A",
        });

        task_graph.use_persistent_buffer(persistent_task_buffer);
        task_graph.use_persistent_buffer(buffer_b);

        task_graph.add_task(daxa::InlineTask::Compute("write buffer b")
                                .writes(buffer_b)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Compute("write buffer b")
                                .writes(buffer_b)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Compute("1 write buffer")
                                .writes(persistent_task_buffer)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Compute("2 read buffer")
                                .reads(persistent_task_buffer)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Compute("3 read buffer")
                                .reads(persistent_task_buffer)
                                .reads(buffer_b)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(daxa::InlineTask::Compute("4 write buffer")
                                .writes(persistent_task_buffer)
                                .executes([](daxa::TaskInterface) {}));

        task_graph.submit({});
        task_graph.complete({});

        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;

        device.wait_idle();
        device.destroy_buffer(buffer);
        device.collect_garbage();
    }

    void optional_attachments()
    {
        // TEST:
        //    1) CREATE image
        //    2) WRITE image
        //    3) READ image
        AppContext app = {};
        // Need to scope the task graphs lifetime.
        // Task graph MUST die before we call wait_idle and collect_garbage.
        auto task_graph = daxa::TaskGraph({
            .device = app.device,
            .record_debug_information = true,
            .name = APPNAME_PREFIX("create-write-read image"),
        });
        // CREATE IMAGE
        auto task_image = task_graph.create_transient_image(daxa::TaskTransientImageInfo{.size = {1, 1, 1}, .name = "task graph tested image"});

        task_graph.add_task(
            daxa::InlineTask::Compute("write image 1")
                .writes(task_image)
                .writes(daxa::NullTaskImage)
                .reads(daxa::NullTaskBuffer)
                .executes([](daxa::TaskInterface) {}));

        task_graph.add_task(
            daxa::InlineTask::Compute("read image 1")
                .samples(task_image)
                .executes([](daxa::TaskInterface) {}));

        task_graph.complete({});
        task_graph.execute({});
        std::cout << task_graph.get_debug_string() << std::endl;
    }
} // namespace tests

auto main() -> i32
{
    std::array dd = {1,2,3,4,5,6,7,8};
    int i = 0;
    daxa::TaskCallback f;
    auto l = [=](daxa::TaskInterface ti) { printf("test: %i\n", dd[0]); };
    f.store(l);

    daxa::Device d;
    daxa::CommandRecorder r;
    daxa::TaskInterface t(d,r);
    f.execute(t);

    tests::head_task_syntax();
    tests::concurrent_read_on_read();
    tests::read_on_readwriteconcurrent();
    tests::simplest();
    tests::execution();
    tests::write_read_image();
    tests::write_read_image_layer();
    tests::create_transfer_read_buffer();
    tests::initial_layout_access();
    tests::tracked_slice_barrier_collapsing();
    tests::correct_read_buffer_task_ordering();
    tests::sharing_persistent_image();
    tests::sharing_persistent_buffer();
    tests::transient_write_aliasing();
    tests::transient_resources();
    tests::shader_integration_inl_use();
    tests::test_concurrent_read_write_buffer();
    tests::test_concurrent_read_write_image();
    tests::test_concurrent_read_write_buffer_cross_graphs();
    tests::mipmapping();
    tests::optional_attachments();
}
